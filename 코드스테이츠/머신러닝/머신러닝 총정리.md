# 머신러닝 지도학습 모델정리
### 선형회귀
```python
from sklearn.linear_model import LinearRegression    # 선형회귀(단순, 다중)   
from sklearn.preprocessing import PolynomialFeatures # 다항 선형회귀
PolynomialFeatures(degree=)

from sklearn.linear.model import RidgeCV, LassoCV   
Ridge = RidgeCV(alphas=alphas, cv=n, random_state=)  # alphas = np.arange(1,100,1)   
Lasso = LassoCV(alphas=alphas, cv=n, random_state=)
```
```python
from sklearn.tree import DecisionTreeRegressor                       # 결정트리 회귀
model = DecisionTreeRegressor(max_depth=, criterion="squared_error") # 분기 수, 평가방법

from sklearn.ensemble import RandomforestRegressor                   # 랜덤포레스트 회귀
model = RandomforestRegressor(max_depth=, n_estimators=, criterion="squared_error") # 분기 수, 기본모델 수, 평가방법
```
```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score   # 회귀평가지표

model.coef_       # 회귀계수(값이 크다고 영향력이 큰 것은 아니다, 단위의 차이일 수 있음) <-> 분류의 feature_importances_
model.intercept_  # y절편
# y = ax + b      # a = coef_  / b = intercept_
```
```python
from sklearn.model_selection import train_test_split   
train_test_split(test_size= , random_state=, Stratify=불균형비율데이터)

from sklearn.model_selection import KFold 
kf = KFold(n_splits=n)   # n개로 분할   
kf.get_n_splits()        # n의 개수 출력   
kf.split(x_train)        # x_train을 cv돌림

from sklearn.model_selection import cross_val_score   
cross_val_score(model, x, y, cv, scoring = "neg_mean_absolute_error") # 작을수록 좋은 평가지표는 neg 붙일 것(mae, mse, rmse) 

from sklearn.feature_selection import f_regression, SelectKBest   
selector = SelectKBest(score_func=f_regression, k=n)                  # 모델, transform으로 변환가능   
# n개의 특성만 사용하여 f_regression 방식으로 종속변수와 독립변수를 계산하는 방식   
selector.get_feature_names_out()                                      # 사용된 특성 목록(array형태)   
```
### 분류
```python
from sklearn.linear_model import LogisticRegression   # 선형회귀에 시그모이드 함수를 씌운 로지스틱회귀
from sklearn.linear_model import LogisticRegressionCV # 검증(cv)와 규제항(Cs)를 추가 할 수있는 로지스틱회귀
Cs = np.arange(1, 100, 1)
logistic_cv = LogisticRegressionCV(Cs=Cs, 
                                   cv=5, 
                                   max_iter=100,
                                   #class_weight = 'balanced’데이터가 불균형 할경우 데이터의 비율을 맞춰줌
                                   )
logistic_cv.C_  # 최적의 Cs값
logistic_cv.Cs_ # Cs의 목록
```
```python
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report # 분류평가지표

from sklearn.metrics import roc_auc_score, roc_curve   # 분류평가지표(0.5보다 작으면 0, 0.5보다 크면 1) 
y_proba = logistic.predict_proba(X_test_ohe)[:,1]      # [:,0] = 0일 확률 ,[:,1] = 1일 확률
roc_auc_score(y_test, y_proba)                         # 1일 확률의 ROC점수
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba) # fpr, tpr, threshold 3가지 값을 출력해줌
```
```python
from sklearn.metrics import plot_confusion_matrix         # 컨퓨전메트릭스
pcm = plot_confusion_matrix(logistic, X_test_ohe, y_test) # 예측값이 아닌 예측값 만들 데이터 넣어야함
pcm.confusion_matrix                                      # plot아닌 행렬데이터(히트맵 위에)[[TN, FP], [FN, TP]]
```
![캡처](https://user-images.githubusercontent.com/110000734/193766235-7b1c8c8f-611e-48a7-b39e-407554c16ef0.JPG)
```python
fpr, tpr, threshold = roc_curve(y_test, y_proba)                  # fpr, tpr, threshold 공식
roc = pd.DataFrame({'TPR':tpr, 'FPR':fpr, 'Threshold':threshold}) # fpr,tpr, threshold 데이터프레임

optimal_index = np.argmax(tpr-fpr)                                # tpr-fpr의 값이 최대인 인덱스
optimal_threshold = roc.Threshold[optimal_index]                  # 최대인 인덱스의 threshold

opt_fpr = roc[roc.Threshold == optimal_threshold]['FPR']          # 최대인 인덱스의 fpr
opt_tpr = roc[roc.Threshold == optimal_threshold]['TPR']          # 최대인 인덱스의 tpr

plt.plot(fpr, tpr)                                                # roc_curve
plt.scatter(opt_fpr, opt_tpr, c='red')                            # 최대인 인덱스의 threshold값
plt.plot(np.arange(0, 1.0, 0.01), np.arange(0, 1.0, 0.01),linestyle = '--')
plt.title('ROC_AUC_CURVE')
plt.xlabel('FPR')
plt.ylabel('TPR', rotation=90)
plt.show();
```
![image](https://user-images.githubusercontent.com/110000734/193826323-f26d32cb-b097-46a8-8cf0-b1605d322fbe.png)

```python
from sklearn.tree import DecisionTreeClassifier         # 의사결정나무 분류
model_dt = DecisionTreeClassifier(random_state=n, 
                                  criterion="entropy",  # 불순도 평가방법 gini
                                  max_depth=n,          # 분기 개수(안쓰면 끝까지)
                                  min_samples_split=,
                                  min_samples_leaf=,
                                  )

!pip install category_encoders
import graphviz                           # 결정트리 시각화(앙상블모델에선 사용X)
from sklearn.tree import export_graphviz 
data = export_graphviz(model, max_depth=n, feature_names=x_train.columns, class_names=['no', 'yes'], filled=True, proportion=True)

display(graphviz.Source(data))
```
```python
from sklearn.ensemble import RandomForestClassifier
# bagging모델(bootstrap -모집단의 샘플에서 복원추출 / aggregating - bootstrap으로 생성된 기본 모델을 합치는 과정[회귀 = 평균 / 분류 = 다수결])
model =RandomForestClassifier(random_state=, 
                              oob_score=True,       # .oob_score_사용여부
                              max_depth=n,          # 분기 개수(안쓰면 끝까지)
                              min_samples_split=,
                              min_samples_leaf=,
                              n_estimators =,       # 기본모델의 수
                              max_features =,       # 분할되는 최대 특성의 수
                              )

model.oob_score_ # oob스코어(각 기본모델의 평균 검증값)
```

```python
!pip install xgboost                       # XGBOOST + make_pipeline(early_stopping 불가)

from sklearn.pipeline import make_pipeline 
from xgboost import XGBClassifier
from category_encoders import OrdinalEncoder
from sklearn.impute import SimpleImputer

pipe = make_pipeline(                 
    OrdinalEncoder(),
    SimpleImputer(strategy="mean"),   # 결측치를 평균값(default)으로 채움 median(중앙값), most_frequent(최빈값)
    XGBClassifier(        
        booster = 'gbtree',           #'gbtree' : decision tree 사용 / 'dart' : 결정트리를 사용하되, 과적합 방지를 위해 일부 트리를 drop / 'gblinear' : 선형모델(지양)
        objective = 'binary:logistic',# 비용함수(목적함수)/ 이진분류  / reg:squarederror = MSE 최소화 회귀 / reg:logistic = Logistic 회귀
        eval_metric='error',          # error = 1 - accuracy 지표를 사용해 평가 / regression: rmse, classification: logloss
        n_estimator = 6,              # 기본모델(weak learner)의 개수
        random_state = 2,
        n_jobs=-1,
        max_depth = 30,                # 분기의 개수 / 너무 크면 과적합(일반적으로 5~12)
        learning_rate = 0.195,         # 기본모델의 반영정도(0~1) / 보통 0.05~0.3 / 너무크면 과적합, 너무 작으면 학습이 느려짐
        min_child_weight = 19,         # leaf 노드에 포함되는 관측치의 수를 결정, 높을수록 기본모델 복잡도감소(과적합시 1,2,4,8처럼 2배로 늘려서 확인)
        colsample_bytree = 0.85,       # 기본모델의 과적합을 막기 위해 columns의 비율을 제한하여 샘플링(0~1) 0.8이 일반적
        subsample = 0.8                # 기본모델의 과적합을 막기 위해 row의 비율을 제한하여 샘플링(0~1) 0.8이 일반적
        scale_pos_weight = (y_train==0).sum()/(y_train==1).sum(), # 0 / 1 의 개수로 가중치를 맞춰줌(class weight의 balenced와 같음)
        reg_lambda = 15,               # 규제항 추가

    )
)
```




```python
!pip install xgboost                                      # XGBOOST + Pipeline(early_stopping가능)

from sklearn.pipeline import Pipeline                     # 대문자P로 시작
from xgboost import XGBClassifier
from category_encoders import OrdinalEncoder
from sklearn.impute import SimpleImputer

early_model = Pipeline(steps=[('enc', OrdinalEncoder()),  # steps에 튜플형태의 리스트로 정리
                              ('impute', SimpleImputer()),# '이름'은 마음대로 지정
                              ('xgbcl', XGBClassifier(        
                                  max_depth = 6,          # 하이퍼파라미터 위에서 갖다 써도 됨
                                  n_estimators = 10000,   # 밑의 early_stopping_rounds가 횟수 제한해줌
                                  random_state = 2,

                              )
)])

enc = early_model[0]                  # Pipeline 꺼내서 인코딩
enc.fit(X_train)
X_train_enc = enc.transform(X_train)
X_val_enc = enc.transform(X_val)
X_test_enc = enc.transform(X_test)

impu = early_model[1]                 # Pipeline 꺼내서 결측치처리
X_train_impu = impu.fit_transform(X_train_enc)
X_val_impu = impu.transform(X_val_enc)
X_test_impu = impu.transform(X_test_enc)

early_list = [(X_train_impu, y_train), (X_val_impu, y_val)]  # EDA한 데이터로 성능개선 확인

early_model.fit(
    X_train_impu,
    y_train,
    xgbcl__eval_set = early_list,     # 성능개선 확인할 데이터(xgbcl__는 위 Pipeline의 xgb모델이름)
    xgbcl__early_stopping_rounds = 30 # 30번 이상 성능개선이 되지 않으면 중지(n_estimaotr의 최적값을 구해줌)
    )
------------------------------------------------------------------------------------------------------------
X_train_pred = early_model.predict(X_train_impu) # 최적 n_estimator에서의 점수확인
X_val_pred = early_model.predict(X_val_impu)
print(f1_score(y_train, X_train_pred), f1_score(y_val, X_val_pred))
------------------------------------------------------------------------------------------------------------    
import matplotlib.pyplot as plt                # 성능결과 시각화

result = early_model[2].evals_result()         # Pipeline의 [2]모델(xgbcl) 성능값
train_error = result['validation_0']["error"]  # early_list 첫번째 데이터 성능
val_error = result['validation_1']["error"]    # early_list 두번째 데이터 성능

plt.plot(train_error, label="Train")
plt.plot(val_error, label="Validation")
plt.ylabel("Classification Error")
plt.xlabel("Model Complexity (n_estimators)")
plt.legend()
```
![image](https://user-images.githubusercontent.com/110000734/193991466-04f3035e-4083-40cc-b580-1e97ccbbfbd3.png)





```python
model.feature_importances_                # 특성중요도
importances = pd.Series(model.feature_importances_ , X_train_ohe.columns) # 특성중요도
importances.sort_values().plot.barh();    # 특성중요도 그래프

pipemodel명.named_steps[‘ordinalencoder'] # 파이프모델의 기능 사용
```

### 데이터 프로파일링
```python
!pip install pandas-profiling==3.1.0         
from pandas_profiling import ProfileReport   # 데이터의 통계치를 평가해줌
profile = ProfileReport(df, minimal=True)    # minimal=False하면 더 많은 것을 분석
profile
```
## 전처리(Preprocessing)
### Regression Model (회귀 모델)  
- 수치간의 곱셈같은 연산으로 예측이 이루어지는 방법 

1. 입력 특성의 크기, 범위 ,분포에 영향을 받는다.
   - scaling, 표준화 필요

2. 모든 데이터를 수치형으로 바꿔줘야 한다.
   - 결측치 처리(제거, 또는 채워주기), 범주형 -> 수치형

3. 비선형적 특성이나 특성간 상호작용을 미리 처리하여 선형관계로 만들어주어야 한다.
   - log, sqrt, 1/c(특성역수), a*b(특성합침)

### Tree Based Model(트리 기반 모델)

1. 입력 특성들의 크기, 범위, 분포에 영향을 받지 않는다.
   -  cultline에 대한 대소관계만 분기해주기때문에 scaling이나 표준화가 불필요

2. 결측치를 반드시 채울 필요는 없다.
   - 결측치 집단을 하나의 집단으로 분기할 수 있기때문
   - xgboost는 결측치 알아서 채워주나, sklearn은 채워주지않음

3. 특성과 타겟 간의 비선형적 관계나 특성 간 상호작용이 자동으로 반영될 수 있다.
   - 트리기반 모델은 선형관계를 가정하지 않아서 유연한 모델링을 할 수 있다.
   - 분기 과정에서 각 특성 간의 상호작용이 저절로 반영

### 결측치 처리

#### 결측치가 생기는 이유
1. 조건부로 존재하는 특성
    - 건강검진에서 산부인과 진료여부(여자는 v, 남자는 None)
2. 설문조사에서 응답자가 의도적으로 비워둔 경우
    - 결측치가 주는 시그널 파악 필요
3. 데이더 다운, 가공,분석 과정에서 엔지니어의 실수

#### 결측치 처리방법
1. 그대로 두기
   - 결측치의 의미를 해석해보는 방법
   - 회귀모델의 경우 결측치 여부의 특성을 bool 형식으로 만들어서 다룸
   - 트리기반모델의 XGBOOST의 경우 핸들링할 필요 없음
   - 트리기반모델의 sklearn은 결측치를 매우 크거나, 작은값으로 설정하여 처리 가능

2. 단일 대표값으로 채우기
    - 가장 흔히 사용하는 방법, 결측치는 해당데이터의 다른 특성들과 무관하게 발생했다는 가정
    - 데이터가 충분히 크거나 결측치가 많지 않을 때(10% 내외) 사용
    - sklearn의 SimpleImputer(strategy = 'mean', 'median', 'most_frequent' : 최빈값(범주형), 'constant' : 숫자로 채우기)
    - fillna(), ffill(), bfill()

```python
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy = 'mean') # 'median'/ 'most_frequent' : 최빈값(범주형) / 'constant' : 숫자로 채우기
# 수동으로 채우려면 fillna(), ffill(), bfill()
```

### 수치형 변수의 전처리

#### 스케일과 형태를 변화시키지 않는 전처리
1. Min-Max Scaling
   - 모든 값을 0과 1사이의 값으로 스케일링
   - 이상치가 크지 않을 때, 특성값의 분포가 전체 범위에서 균등분포일 때(Uniform Distribution) 사용
   - 사람들의 나이정보나 딥러닝의 이미지데이터 등에 사용

2. Standardization(StandardScaler)
   - 특성의 평균을 0, 표준편차를 1로 조정
   - 이상치에 덜 민감하고, 이상치의 분포를 파악하기 어려울 때 사용(주로 첫 번째 스케일링으로 많이 사용)
   - 분포자체는 변화하지 않음

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler = StandardScaler()
scaler = MinMaxScaler()
```

#### 스케일과 형태를 변화시키는 전처리
1. Clipping
   - 특정 범위를 넘어서는 값은 경계값으로 변환
   - df['columns'].clip(lower=0, upper=100) = 0보다 작은 값은 0, 100보다 큰값은 100으로 변환
   - 분포 전체의 모양은 변화하지 않고, 이상치만 변화

2. Log 변환
   - 다수의 값들이 제한된 범위내에만 존재하고 특정 값들이 큰 형태의 분포에서 사용(right skewed data : 오른쪽으로 긴꼬리)
   - 모든 값이 음수(-)가 아닐 때만 사용(음수를 사용하려면 적절한 값을 더해주어 사용)
   - 인터넷 기사의 댓글수, 파레토법칙, power law

3. Bucketing
   - 수치형 변수를 범주형 변수처럼 다루는 방법 (10대, 20대. 30대..)
   - 각 값들이 속하는 범위를 미리 지정해주고 숫자로 나타낸다.

```python
from sklearn.preprocessing import KbinDiscretizer

kbd = KBinsDiscretizer(n_bins=8,           # 쪼갤 개수
                       encode='ordinal',   # 인코딩 방식 ,'onehot'
                       strategy='quantile' # 범위 내에 동일한 수의 데이터가 들어가도록 / 'kmeans' 
                                           # 'uniform' 동일한 간격을 갖도록
)
```

4. Rank
   - 전체 데이터에서의 순위나 percentile로 변환(pandas의 rank)
   - 값들 간의 거리 정보들이 복구할 수 없는 형태로 대소관계만 남음
   - 이상치에 민감하지 않으며, 어떠한 분포를 갖는 특성이든 균등분포(Uniform Distribution)로 변환
   - 추가데이터에 대한 분석 불가(기존데이터만 분석)

```python
df['columns'].rank(pct=True) # True = 퍼센트, False= 랭킹

````
5. QuantileTransformer
   - Rank에서 추가되는 데이터에 대한 분석이 하고 싶다면 사용
   - 변환하면 대부분 균등분포(Uniform Distribution)로 변환

```python
from sklearn.preprocessing import QuantileTransformer

tfer = QuantileTransformer()
tfer.fit_transform(df['columns'])
```

### 범주형 변수의 전처리
-  !pip install category_encoders   
1. OneHotEncoder
   - 명목형 변수에서 각 특성의 값에 해당하지않는다/해당한다(0, 1)로 변환 
   - 특성의트리기반 모델에서는 특성의 정보를 분산시켜 사용하지않음
   - High Cadinality data에서는 차원이 너무 커져서 효율성이 떨어짐

```python
from category_encoders import OneHotEncoder              # 범주형특성의 0과 1인 특성 nunique()수 만큼 생성
ohe = OneHotEncoder(cols='column', use_cat_name=bool)    # 적용할 컬럼(안쓰면 범주형(카테고리,오브젝트) 모두 변경)  / 특성이름 x_colname으로 할건지, x_1 , x_2로 할건지 
ohe.category_mapping                                     # 인코딩 특성 확인
```

2. OrdinalEncoder
   - 범주형 특성의 값을 서로 다른 정수 값으로 변환
   - 순서형 변수에서 주로 사용
   - 회귀모델에는 적절하지 않음(범주형 데이터가 양적 대소관계를 갖는 것 처럼 간주되기 때문)
   - 트리기반모델에서는 잘 작동함

```python
from category_encoders import OrdinalEncoder # 범주형 데이터를 숫자형으로 바꿔줌(a, b, c) -> (1, 2, 3)
enc = OrdinalEncoder()   
enc.fit_transform(df['columns'])
```
```python
genh_encoding = {'Poor':1, 'Fair':2, 'Excellent':3, 'Good':4, 'Very good':5} # 범주형 직접 인코딩
df['GenHealth'] = df['GenHealth'].replace(genh_encoding) 
```

3. CountEncoder
   - 각 카테고리의 등장 빈도가 중요한 경우
   - 각 특성값의 등장 빈도가 값을 대체
   - 구현이나 해석이 쉽고 특성의 차원을 늘리지 않음
   - 동일한 빈도를 갖는 값이 구분되지 않음
   - 새로운 값에 대한 대응이 안됨(미리 정해둔 값이나 결측치로 인코딩해줌)

```python
from category_encoders import CountEncoder

count_encoder = CountEncoder(normalize=True) # True = 비율/ False = 빈도수
count_encoder.fit_transform(df['columns'])
```

4. Target Encoder
   - 각 특성값들을 갖는 타겟의 평균으로 변환
   - 해석이 쉬우며 특성의 차원을 늘리지 않음
   - 특성값과 타겟값의 직접적인 관계를 모델링하여 중요한 정보를 제공하는 장점 -> But, 과적합 문제
   - 동일한 평균 타겟 값을 갖는 경우 구분이 안됨
   - 해당 특성값이 적을 때 misleading을 하게 됨(특성값이 1개뿐이면 그 타겟값이 평균값이 되어버림) -> 라이브러리에서 해결해줌

```python
from category_encoders import TargetEncoder

target_encoder = TargetEncoder()

target_encoder.fit_transform(df['columns'], y=df[target]) # 특성과 타겟을 함께 넣어줘야함
```

5. Label Encoder(찾아보기)
