# 머신러닝 지도학습 모델정리
### 선형회귀
```python
from sklearn.linear_model import LinearRegression    # 선형회귀(단순, 다중)   
from sklearn.preprocessing import PolynomialFeatures # 다항 선형회귀
PolynomialFeatures(degree=)

from sklearn.linear.model import RidgeCV, LassoCV   
Ridge = RidgeCV(alphas=alphas, cv=n, random_state=)  # alphas = np.arange(1,100,1)   
Lasso = LassoCV(alphas=alphas, cv=n, random_state=)
```
```python
from sklearn.tree import DecisionTreeRegressor                       # 결정트리 회귀
model = DecisionTreeRegressor(max_depth=, criterion="squared_error") # 분기 수, 평가방법

from sklearn.ensemble import RandomforestRegressor                   # 랜덤포레스트 회귀
model = RandomforestRegressor(max_depth=, n_estimators=, criterion="squared_error") # 분기 수, 기본모델 수, 평가방법
```
```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score   # 회귀평가지표

model.coef_       # 회귀계수(값이 크다고 영향력이 큰 것은 아니다, 단위의 차이일 수 있음) <-> 분류의 feature_importances_
model.intercept_  # y절편
# y = ax + b      # a = coef_  / b = intercept_
```
```python
from sklearn.model_selection import train_test_split   
train_test_split(test_size= , random_state=, Stratify=불균형비율데이터)

from sklearn.model_selection import KFold 
kf = KFold(n_splits=n)   # n개로 분할   
kf.get_n_splits()        # n의 개수 출력   
kf.split(x_train)        # x_train을 cv돌림

from sklearn.model_selection import cross_val_score   
cross_val_score(model, x, y, cv, scoring = "neg_mean_absolute_error") # 작을수록 좋은 평가지표는 neg 붙일 것(mae, mse, rmse) 

from sklearn.feature_selection import f_regression, SelectKBest   
selector = SelectKBest(score_func=f_regression, k=n)                  # 모델, transform으로 변환가능   
# n개의 특성만 사용하여 f_regression 방식으로 종속변수와 독립변수를 계산하는 방식   
selector.get_feature_names_out()                                      # 사용된 특성 목록(array형태)   
```
### 분류
```python
from sklearn.linear_model import LogisticRegression   # 선형회귀에 시그모이드 함수를 씌운 로지스틱회귀
from sklearn.linear_model import LogisticRegressionCV # 검증(cv)와 규제항(Cs)를 추가 할 수있는 로지스틱회귀
Cs = np.arange(1, 100, 1)
logistic_cv = LogisticRegressionCV(Cs=Cs, 
                                   cv=5, 
                                   max_iter=100,
                                   #class_weight = 'balanced’데이터가 불균형 할경우 데이터의 비율을 맞춰줌
                                   )
logistic_cv.C_  # 최적의 Cs값
logistic_cv.Cs_ # Cs의 목록
```
```python
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report # 분류평가지표

from sklearn.metrics import roc_auc_score, roc_curve   # 분류평가지표(0.5보다 작으면 0, 0.5보다 크면 1) 
y_proba = logistic.predict_proba(X_test_ohe)[:,1]      # [:,0] = 0일 확률 ,[:,1] = 1일 확률
roc_auc_score(y_test, y_proba)                         # 1일 확률의 ROC점수
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba) # fpr, tpr, threshold 3가지 값을 출력해줌
```
```python
from sklearn.metrics import plot_confusion_matrix         # 컨퓨전메트릭스
pcm = plot_confusion_matrix(logistic, X_test_ohe, y_test) # 예측값이 아닌 예측값 만들 데이터 넣어야함
pcm.confusion_matrix                                      # plot아닌 행렬데이터(히트맵 위에)[[TN, FP], [FN, TP]]
```
![캡처](https://user-images.githubusercontent.com/110000734/193766235-7b1c8c8f-611e-48a7-b39e-407554c16ef0.JPG)
```python
fpr, tpr, threshold = roc_curve(y_test, y_proba)                  # fpr, tpr, threshold 공식
roc = pd.DataFrame({'TPR':tpr, 'FPR':fpr, 'Threshold':threshold}) # fpr,tpr, threshold 데이터프레임

optimal_index = np.argmax(tpr-fpr)                                # tpr-fpr의 값이 최대인 인덱스
optimal_threshold = roc.Threshold[optimal_index]                  # 최대인 인덱스의 threshold

opt_fpr = roc[roc.Threshold == optimal_threshold]['FPR']          # 최대인 인덱스의 fpr
opt_tpr = roc[roc.Threshold == optimal_threshold]['TPR']          # 최대인 인덱스의 tpr

plt.plot(fpr, tpr)                                                # roc_curve
plt.scatter(opt_fpr, opt_tpr, c='red')                            # 최대인 인덱스의 threshold값
plt.plot(np.arange(0, 1.0, 0.01), np.arange(0, 1.0, 0.01),linestyle = '--')
plt.title('ROC_AUC_CURVE')
plt.xlabel('FPR')
plt.ylabel('TPR', rotation=90)
plt.show();
```
![image](https://user-images.githubusercontent.com/110000734/193826323-f26d32cb-b097-46a8-8cf0-b1605d322fbe.png)

```python
from sklearn.tree import DecisionTreeClassifier         # 의사결정나무 분류
model_dt = DecisionTreeClassifier(random_state=n, 
                                  criterion="entropy",  # 불순도 평가방법 gini
                                  max_depth=n,          # 분기 개수(안쓰면 끝까지)
                                  min_samples_split=,
                                  min_samples_leaf=,
                                  )

!pip install category_encoders
import graphviz                           # 결정트리 시각화(앙상블모델에선 사용X)
from sklearn.tree import export_graphviz 
data = export_graphviz(model, max_depth=n, feature_names=x_train.columns, class_names=['no', 'yes'], filled=True, proportion=True)

display(graphviz.Source(data))
```
```python
from sklearn.ensemble import RandomForestClassifier
# bagging모델(bootstrap -모집단의 샘플에서 복원추출 / aggregating - bootstrap으로 생성된 기본 모델을 합치는 과정[회귀 = 평균 / 분류 = 다수결])
model =RandomForestClassifier(random_state=, 
                              oob_score=True,       # .oob_score_사용여부
                              max_depth=n,          # 분기 개수(안쓰면 끝까지)
                              min_samples_split=,
                              min_samples_leaf=,
                              n_estimators =,       # 기본모델의 수
                              max_features =,       # 분할되는 최대 특성의 수
                              )

model.oob_score_ # oob스코어(각 기본모델의 평균 검증값)
```
```python
model.feature_importances_                # 특성중요도
importances = pd.Series(model.feature_importances_ , X_train_ohe.columns) # 특성중요도
importances.sort_values().plot.barh();    # 특성중요도 그래프

pipemodel명.named_steps[‘ordinalencoder'] # 파이프모델의 기능 사용
```

### 모델구축
```python
from sklearn.pipeline import make_pipeline                     # 메이크 파이프라인
 
from sklearn.preprocessing import StandardScaler, MinMaxScaler # 표준화(분포 변화 x)

from sklearn.impute import SimpleImputer
Imputer = SimpleImputer() # 결측치 채우기(default = mean) median 등등..

!pip install category_encoders                                 # 인코딩
from category_encoders import OneHotEncoder                    # 범주형특성의 0과 1인 특성 nunique()수 만큼 생성
ohe = OneHotEncoder(cols='column', use_cat_name=bool)   # 적용할 컬럼(안쓰면 범주형(카테고리,오브젝트) 모두 변경)  / 특성이름 x_colname으로 할건지, x_1 , x_2로 할건지 
ohe.category_mapping                                           # 인코딩 특성 확인

from category_encoders import OrdinalEncoder # 범주형 데이터를 숫자형으로 바꿔줌(a, b, c) -> (1, 2, 3)
enc = OrdinalEncoder()   

genh_encoding = {'Poor':1, 'Fair':2, 'Excellent':3, 'Good':4, 'Very good':5} # 범주형 직접 인코딩
df['GenHealth'] = df['GenHealth'].replace(genh_encoding) 

!pip install pandas-profiling==3.1.0
from pandas_profiling import ProfileReport   # df의 데이터 통계치를 평가해줌
profile = ProfileReport(df, minimal=True)    # minimal=False하면 더 많은 것을 분석
profile
```
