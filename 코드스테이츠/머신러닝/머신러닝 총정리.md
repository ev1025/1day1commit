# 머신러닝 지도학습 모델정리
### 선형회귀
```python
from sklearn.linear_model import LinearRegression    # 선형회귀(단순, 다중)   
from sklearn.preprocessing import PolynomialFeatures # 다항 선형회귀
PolynomialFeatures(degree=)

from sklearn.linear.model import RidgeCV, LassoCV   
Ridge = RidgeCV(alphas=alphas, cv=n, random_state=)  # alphas = np.arange(1,100,1)   
Lasso = LassoCV(alphas=alphas, cv=n, random_state=)
```
```python
from sklearn.tree import DecisionTreeRegressor                       # 결정트리 회귀
model = DecisionTreeRegressor(max_depth=, criterion="squared_error") # 분기 수, 평가방법

from sklearn.ensemble import RandomforestRegressor                   # 랜덤포레스트 회귀
model = RandomforestRegressor(max_depth=, n_estimators=, criterion="squared_error") # 분기 수, 기본모델 수, 평가방법
```
```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score   # 회귀평가지표

model.coef_       # 회귀계수(값이 크다고 영향력이 큰 것은 아니다, 단위의 차이일 수 있음) <-> 분류의 feature_importances_
model.intercept_  # y절편
# y = ax + b      # a = coef_  / b = intercept_
```
```python
from sklearn.model_selection import train_test_split   
train_test_split(test_size= , random_state=, Stratify=불균형비율데이터)

from sklearn.model_selection import KFold 
kf = KFold(n_splits=n)   # n개로 분할   
kf.get_n_splits()        # n의 개수 출력   
kf.split(x_train)        # x_train을 cv돌림

from sklearn.model_selection import cross_val_score   
cross_val_score(model, x, y, cv, scoring = "neg_mean_absolute_error") # 작을수록 좋은 평가지표는 neg 붙일 것(mae, mse, rmse) 

from sklearn.feature_selection import f_regression, SelectKBest   
selector = SelectKBest(score_func=f_regression, k=n)                  # 모델, transform으로 변환가능   
# n개의 특성만 사용하여 f_regression 방식으로 종속변수와 독립변수를 계산하는 방식   
selector.get_feature_names_out()                                      # 사용된 특성 목록(array형태)   
```
### 분류
```python
from sklearn.linear_model import LogisticRegression   # 선형회귀에 시그모이드 함수를 씌운 로지스틱회귀
from sklearn.linear_model import LogisticRegressionCV # 검증(cv)와 규제항(Cs)를 추가 할 수있는 로지스틱회귀
Cs = np.arange(1, 100, 1)
logistic_cv = LogisticRegressionCV(Cs=Cs, 
                                   cv=5, 
                                   max_iter=100,
                                   #class_weight = 'balanced’데이터가 불균형 할경우 데이터의 비율을 맞춰줌
                                   )
logistic_cv.C_  # 최적의 Cs값
logistic_cv.Cs_ # Cs의 목록
```
```python
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report # 분류평가지표

from sklearn.metrics import roc_auc_score, roc_curve   # 분류평가지표(0.5보다 작으면 0, 0.5보다 크면 1) 
y_proba = logistic.predict_proba(X_test_ohe)[:,1]      # [:,0] = 0일 확률 ,[:,1] = 1일 확률
roc_auc_score(y_test, y_proba)                         # 1일 확률의 ROC점수
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba) # fpr, tpr, threshold 3가지 값을 출력해줌
```
```python
from sklearn.metrics import plot_confusion_matrix         # 컨퓨전메트릭스
pcm = plot_confusion_matrix(logistic, X_test_ohe, y_test) # 예측값이 아닌 예측값 만들 데이터 넣어야함
pcm.confusion_matrix                                      # plot아닌 행렬데이터(히트맵 위에)[[TN, FP], [FN, TP]]
```
![캡처](https://user-images.githubusercontent.com/110000734/193766235-7b1c8c8f-611e-48a7-b39e-407554c16ef0.JPG)
```python
fpr, tpr, threshold = roc_curve(y_test, y_proba)                  # fpr, tpr, threshold 공식
roc = pd.DataFrame({'TPR':tpr, 'FPR':fpr, 'Threshold':threshold}) # fpr,tpr, threshold 데이터프레임

optimal_index = np.argmax(tpr-fpr)                                # tpr-fpr의 값이 최대인 인덱스
optimal_threshold = roc.Threshold[optimal_index]                  # 최대인 인덱스의 threshold

opt_fpr = roc[roc.Threshold == optimal_threshold]['FPR']          # 최대인 인덱스의 fpr
opt_tpr = roc[roc.Threshold == optimal_threshold]['TPR']          # 최대인 인덱스의 tpr

plt.plot(fpr, tpr)                                                # roc_curve
plt.scatter(opt_fpr, opt_tpr, c='red')                            # 최대인 인덱스의 threshold값
plt.plot(np.arange(0, 1.0, 0.01), np.arange(0, 1.0, 0.01),linestyle = '--')
plt.title('ROC_AUC_CURVE')
plt.xlabel('FPR')
plt.ylabel('TPR', rotation=90)
plt.show();
```
![image](https://user-images.githubusercontent.com/110000734/193826323-f26d32cb-b097-46a8-8cf0-b1605d322fbe.png)

```python
from sklearn.tree import DecisionTreeClassifier         # 의사결정나무 분류
model_dt = DecisionTreeClassifier(random_state=n, 
                                  criterion="entropy",  # 불순도 평가방법 gini
                                  max_depth=n,          # 분기 개수(안쓰면 끝까지)
                                  min_samples_split=,
                                  min_samples_leaf=,
                                  )

!pip install category_encoders
import graphviz                           # 결정트리 시각화(앙상블모델에선 사용X)
from sklearn.tree import export_graphviz 
data = export_graphviz(model, max_depth=n, feature_names=x_train.columns, class_names=['no', 'yes'], filled=True, proportion=True)

display(graphviz.Source(data))
```
```python
from sklearn.ensemble import RandomForestClassifier
# bagging모델(bootstrap -모집단의 샘플에서 복원추출 / aggregating - bootstrap으로 생성된 기본 모델을 합치는 과정[회귀 = 평균 / 분류 = 다수결])
model =RandomForestClassifier(random_state=, 
                              oob_score=True,       # .oob_score_사용여부
                              max_depth=n,          # 분기 개수(안쓰면 끝까지)
                              min_samples_split=,
                              min_samples_leaf=,
                              n_estimators =,       # 기본모델의 수
                              max_features =,       # 분할되는 최대 특성의 수
                              )

model.oob_score_ # oob스코어(각 기본모델의 평균 검증값)
```

```python
!pip install xgboost                       # XGBOOST + make_pipeline(early_stopping 불가)

from sklearn.pipeline import make_pipeline 
from xgboost import XGBClassifier
from category_encoders import OrdinalEncoder
from sklearn.impute import SimpleImputer

pipe = make_pipeline(                 
    OrdinalEncoder(),
    SimpleImputer(strategy="mean"),   # 결측치를 평균값(default)으로 채움 median(중앙값), most_frequent(최빈값)
    XGBClassifier(        
        booster = 'gbtree',           #'gbtree' : decision tree 사용 / 'dart' : 결정트리를 사용하되, 과적합 방지를 위해 일부 트리를 drop / 'gblinear' : 선형모델(지양)
        objective = 'binary:logistic',# 비용함수(목적함수)/ 이진분류  / reg:squarederror = MSE 최소화 회귀 / reg:logistic = Logistic 회귀
        eval_metric='error',          # error = 1 - accuracy 지표를 사용해 평가 / regression: rmse, classification: logloss
        n_estimator = 6,              # 기본모델(weak learner)의 개수
        random_state = 2,
        n_jobs=-1,
        max_depth = 30,                # 분기의 개수 / 너무 크면 과적합(일반적으로 5~12)
        learning_rate = 0.195,         # 기본모델의 반영정도(0~1) / 보통 0.05~0.3 / 너무크면 과적합, 너무 작으면 학습이 느려짐
        min_child_weight = 19,         # leaf 노드에 포함되는 관측치의 수를 결정, 높을수록 기본모델 복잡도감소(과적합시 1,2,4,8처럼 2배로 늘려서 확인)
        colsample_bytree = 0.85,       # 기본모델의 과적합을 막기 위해 columns의 비율을 제한하여 샘플링(0~1) 0.8이 일반적
        subsample = 0.8                # 기본모델의 과적합을 막기 위해 row의 비율을 제한하여 샘플링(0~1) 0.8이 일반적
        scale_pos_weight = (y_train==0).sum()/(y_train==1).sum(), # 0 / 1 의 개수로 가중치를 맞춰줌(class weight의 balenced와 같음)
        reg_lambda = 15,               # 규제항 추가

    )
)
```




```python
!pip install xgboost                                      # XGBOOST + Pipeline(early_stopping가능)

from sklearn.pipeline import Pipeline                     # 대문자P로 시작
from xgboost import XGBClassifier
from category_encoders import OrdinalEncoder
from sklearn.impute import SimpleImputer

early_model = Pipeline(steps=[('enc', OrdinalEncoder()),  # steps에 튜플형태의 리스트로 정리
                              ('impute', SimpleImputer()),# '이름'은 마음대로 지정
                              ('xgbcl', XGBClassifier(        
                                  max_depth = 6,          # 하이퍼파라미터 위에서 갖다 써도 됨
                                  n_estimators = 10000,   # 밑의 early_stopping_rounds가 횟수 제한해줌
                                  random_state = 2,

                              )
)])

enc = early_model[0]                  # Pipeline 꺼내서 인코딩
enc.fit(X_train)
X_train_enc = enc.transform(X_train)
X_val_enc = enc.transform(X_val)
X_test_enc = enc.transform(X_test)

impu = early_model[1]                 # Pipeline 꺼내서 결측치처리
X_train_impu = impu.fit_transform(X_train_enc)
X_val_impu = impu.transform(X_val_enc)
X_test_impu = impu.transform(X_test_enc)

early_list = [(X_train_impu, y_train), (X_val_impu, y_val)]  # EDA한 데이터로 성능개선 확인

early_model.fit(
    X_train_impu,
    y_train,
    xgbcl__eval_set = early_list,     # 성능개선 확인할 데이터(xgbcl__는 위 Pipeline의 xgb모델이름)
    xgbcl__early_stopping_rounds = 30 # 30번 이상 성능개선이 되지 않으면 중지(n_estimaotr의 최적값을 구해줌)
    )
```





```python
model.feature_importances_                # 특성중요도
importances = pd.Series(model.feature_importances_ , X_train_ohe.columns) # 특성중요도
importances.sort_values().plot.barh();    # 특성중요도 그래프

pipemodel명.named_steps[‘ordinalencoder'] # 파이프모델의 기능 사용
```

### 모델구축
```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler # 표준화(분포 변화 x)

from sklearn.impute import SimpleImputer
Imputer = SimpleImputer() # 결측치 채우기(default = mean) median 등등..

!pip install category_encoders                                 # 인코딩
from category_encoders import OneHotEncoder                    # 범주형특성의 0과 1인 특성 nunique()수 만큼 생성
ohe = OneHotEncoder(cols='column', use_cat_name=bool)   # 적용할 컬럼(안쓰면 범주형(카테고리,오브젝트) 모두 변경)  / 특성이름 x_colname으로 할건지, x_1 , x_2로 할건지 
ohe.category_mapping                                           # 인코딩 특성 확인

from category_encoders import OrdinalEncoder # 범주형 데이터를 숫자형으로 바꿔줌(a, b, c) -> (1, 2, 3)
enc = OrdinalEncoder()   

genh_encoding = {'Poor':1, 'Fair':2, 'Excellent':3, 'Good':4, 'Very good':5} # 범주형 직접 인코딩
df['GenHealth'] = df['GenHealth'].replace(genh_encoding) 

!pip install pandas-profiling==3.1.0
from pandas_profiling import ProfileReport   # df의 데이터 통계치를 평가해줌
profile = ProfileReport(df, minimal=True)    # minimal=False하면 더 많은 것을 분석
profile
```
